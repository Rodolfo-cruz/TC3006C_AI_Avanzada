# TC3006C_AI_Avanzada
Repositorio para almacenar archivos del portafolio de implementación, de análsis y del reto

# Valhalla Challenge: Entrega 1

Archivos a revisar para esta entrega y su ubicación dentro del repositorio:

1. Jupyter Notebook (archivo .ipynb): ubicado en la ruta TC3006C_AI_Avanzada/Portafolio_Implementacion/Machine_Learning/Valhalla_Entrega1/Valhalla.ipynb
2. PDF del Jupyter Notebook: ubicado en la ruta TC3006C_AI_Avanzada/Portafolio_Implementacion/Machine_Learning/Valhalla_Entrega1/Valhalla.pdf

Nota: No tomar en cuenta los archivos de texto (.txt) a lo largo de la ruta de ubicación de los archivos, ya que sirvieron solamente para crear 
      las carpetas de la ruta, pero no hay contenido asociado al proyecto en ellos.

# Normatividad del reto elegido del artículo (toma de decisiones algorítmica):

**Nota:** No tomar en cuenta los archivos de texto .txt a lo largo de la ruta de ubicación de los archivos, ya que solamente sirvieron para generar 
          las carpetas de las rutas, pero dichos archivos no tienen contenido relacionado a la concentración.

**Archivos a revisar y su ubicación en el repositorio:**

* Ensayo en formato pdf: ubicado en la ruta: TC3006C_AI_Avanzada/Portafolio_Analisis/Analisis_contexto_normatividad/Entregable1/Analisis_contexto_normatividad.pdf

  ## Normatividad del reto:

1. **Normatividad asociada a falta de confiabilidad de los algoritmos:** Asegurar que los datos con los que se entrenan los algoritmos sean de alta calidad(libres de errores
   o inconsistencias en su estructura), con el fin de evitar que debido a las inconsistencias en los datos, los algoritmos no logren aprender adecuadamente y como consecuencia,
   generen predicciones erradas que se alejen mucho de la realidad y por tanto carezcan de confiabilidad.

2. **Normatividad asociada a la falta de implicación humana en los algoritmos:** Los métodos de funcionamiento de los algoritmos de IA deberían ser accesibles para aquellas personas
   a las que les afecta de forma significativa la decisión tomada por éstos para poder anticipar lo que pasará si realizan algunas acciones en específico o si por el contrario no las
   llevan a cabo con el fin de preparase para experimentar las consecuencias de dichas acciones u omisiones, no obstante, al no involucrar personas en la toma de decisiones algorítmica,
   las decisiones resultantes de dichos algoritmos no son claramente comprendidas en su gran mayoría, lo cual puede ocasionar que en caso de presentarse algún inconveniente con el impacto
   de alguna decisión algorítmica sobre la vida de las personas, las personas afectadas no serán capaces de comprender las razones detrás de dicha decisión tomada por el algoritmo y como
   consecuencia, sus alternativas de acción para mitigar o evitar el efecto de la decisión tomada por el algoritmo serán muy limitadas.

3. **Normatividad asociada a la incapacidad de rendir cuentas sobre las decisiones algorítmicas:** La responsabilidad de las decisiones tomadas por algoritmos de inteligencia artificial idealmente
   le correspondería a los desarrolladores y/o programadores de los mismos, dado que son las personas que en pimer lugar dieron las instrucciones a dichos algoritmos sobre cuáles acciones realizar
   en caso de que sucedan diferentes situaciones, por lo que en caso de que el algoritmo tome una decisión que afecte profundamente a una persona en algún contexto específico de su vida cotidiana,
   a dicha persona afectada le será muy complicado poder saber quién es el responsable de la configuración de dicho algoritmo de IA para así poder actuar incluso legalmente si la situación lo
   ameritara, no obstante al dejar que los algoritmos tomen decisiones, ya sean de bajo, medio o alto impacto, es muy difícil encontrar a las personas responsables de dichas
   decisiones, por lo cual muchas veces no es posible actuar para mitigar el efecto de las decisiones automatizadas al desconocer muchos de los detalles que se encuentran detrás
   de la decisión tomada por un algoritmo, constituyendo así una falta a los principios éticos de nuestra sociedad, ya que los afectados no podrán contar con la información necesaria
   sobre el funcionamiento del algoritmo y detalles sobre cómo éste tomó una determinada decisión que afecte significativamente su vida en algún contexto, violando así el derecho a la información
   que tenemos como personas.
    
